{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec10301c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage.io import imread\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.core import Lambda, Flatten, Dense\n",
    "from keras.initializers import glorot_uniform\n",
    "\n",
    "from tensorflow.keras.layers import Layer\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import numpy.random as rng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ce6fd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder=\"./python/images_background/images_background/\"\n",
    "val_folder=\"./python/images_evaluation/images_evaluation/\"\n",
    "save_path=\"./data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea8f7fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadimgs(path,n = 0):\n",
    "    '''\n",
    "    path => Path of train directory or test directory\n",
    "    '''\n",
    "    X=[]\n",
    "    y = []\n",
    "    cat_dict = {}\n",
    "    lang_dict = {}\n",
    "    curr_y = n\n",
    "    # we load every alphabet seperately so we can isolate them later\n",
    "    for alphabet in os.listdir(path):\n",
    "        print(\"loading alphabet: \" + alphabet)\n",
    "        lang_dict[alphabet] = [curr_y,None]\n",
    "        alphabet_path = os.path.join(path,alphabet)\n",
    "        # every letter/category has it's own column in the array, so  load seperately\n",
    "        for letter in os.listdir(alphabet_path):\n",
    "            cat_dict[curr_y] = (alphabet, letter)\n",
    "            category_images=[]\n",
    "            letter_path = os.path.join(alphabet_path, letter)\n",
    "            # read all the images in the current category\n",
    "            for filename in os.listdir(letter_path):\n",
    "                image_path = os.path.join(letter_path, filename)\n",
    "                image = imread(image_path)\n",
    "                category_images.append(image)\n",
    "                y.append(curr_y)\n",
    "            try:\n",
    "                X.append(np.stack(category_images))\n",
    "            # edge case  - last one\n",
    "            except ValueError as e:\n",
    "                print(e)\n",
    "                print(\"error - category_images:\", category_images)\n",
    "            curr_y += 1\n",
    "            lang_dict[alphabet][1] = curr_y - 1\n",
    "    y = np.vstack(y)\n",
    "    X = np.stack(X)\n",
    "    print(\"Loading Completed\")\n",
    "    return X,y,lang_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2821fe52",
   "metadata": {},
   "source": [
    "# Loading the train images into tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fde4d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading alphabet: Alphabet_of_the_Magi\n",
      "loading alphabet: Anglo-Saxon_Futhorc\n",
      "loading alphabet: Arcadian\n",
      "loading alphabet: Armenian\n",
      "loading alphabet: Asomtavruli_(Georgian)\n",
      "loading alphabet: Balinese\n",
      "loading alphabet: Bengali\n",
      "loading alphabet: Blackfoot_(Canadian_Aboriginal_Syllabics)\n",
      "loading alphabet: Braille\n",
      "loading alphabet: Burmese_(Myanmar)\n",
      "loading alphabet: Cyrillic\n",
      "loading alphabet: Early_Aramaic\n",
      "loading alphabet: Futurama\n",
      "loading alphabet: Grantha\n",
      "loading alphabet: Greek\n",
      "loading alphabet: Gujarati\n",
      "loading alphabet: Hebrew\n",
      "loading alphabet: Inuktitut_(Canadian_Aboriginal_Syllabics)\n",
      "loading alphabet: Japanese_(hiragana)\n",
      "loading alphabet: Japanese_(katakana)\n",
      "loading alphabet: Korean\n",
      "loading alphabet: Latin\n",
      "loading alphabet: Malay_(Jawi_-_Arabic)\n",
      "loading alphabet: Mkhedruli_(Georgian)\n",
      "loading alphabet: N_Ko\n",
      "loading alphabet: Ojibwe_(Canadian_Aboriginal_Syllabics)\n",
      "loading alphabet: Sanskrit\n",
      "loading alphabet: Syriac_(Estrangelo)\n",
      "loading alphabet: Tagalog\n",
      "loading alphabet: Tifinagh\n"
     ]
    }
   ],
   "source": [
    "X,y,c=loadimgs(train_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0ba998",
   "metadata": {},
   "source": [
    "# Saving the train tensors on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d68ff05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(save_path,\"train.pickle\"), \"wb\") as f:\n",
    "    pickle.dump((X,c),f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c288732a",
   "metadata": {},
   "source": [
    "# Loading the validation images into tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46f0ad32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading alphabet: Angelic\n",
      "loading alphabet: Atemayar_Qelisayer\n",
      "loading alphabet: Atlantean\n",
      "loading alphabet: Aurek-Besh\n",
      "loading alphabet: Avesta\n",
      "loading alphabet: Ge_ez\n",
      "loading alphabet: Glagolitic\n",
      "loading alphabet: Gurmukhi\n",
      "loading alphabet: Kannada\n",
      "loading alphabet: Keble\n",
      "loading alphabet: Malayalam\n",
      "loading alphabet: Manipuri\n",
      "loading alphabet: Mongolian\n",
      "loading alphabet: Old_Church_Slavonic_(Cyrillic)\n",
      "loading alphabet: Oriya\n",
      "loading alphabet: Sylheti\n",
      "loading alphabet: Syriac_(Serto)\n",
      "loading alphabet: Tengwar\n",
      "loading alphabet: Tibetan\n",
      "loading alphabet: ULOG\n",
      "Loading Completed\n"
     ]
    }
   ],
   "source": [
    "Xval,yval,cval=loadimgs(val_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdfdaf7",
   "metadata": {},
   "source": [
    "# Saving the validation tesnor on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8772cc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(save_path,\"val.pickle\"), \"wb\") as f:\n",
    "    pickle.dump((Xval,cval),f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86335fbd",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b18b67a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(shape, dtype=None):\n",
    "    \"\"\"\n",
    "        The paper, http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf\n",
    "        suggests to initialize CNN layer weights with mean as 0.0 and standard deviation of 0.01\n",
    "    \"\"\"\n",
    "    return np.random.normal(loc = 0.0, scale = 1e-2, size = shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0efdbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_bias(shape, dtype=None):\n",
    "    \"\"\"\n",
    "        The paper, http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf\n",
    "        suggests to initialize CNN layer bias with mean as 0.5 and standard deviation of 0.01\n",
    "    \"\"\"\n",
    "    return np.random.normal(loc = 0.5, scale = 1e-2, size = shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80f818e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_siamese_model(input_shape):\n",
    "    \"\"\"\n",
    "        Model architecture based on the one provided in: http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the tensors for the two input images\n",
    "    left_input = Input(input_shape)\n",
    "    right_input = Input(input_shape)\n",
    "    \n",
    "    # Convolutional Neural Network\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(16, (3,3), activation='relu', input_shape=input_shape,\n",
    "                   kernel_initializer=initialize_weights, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    model.add(Conv2D(32, (3,3), activation='relu',\n",
    "                     kernel_initializer=initialize_weights,\n",
    "                     bias_initializer=initialize_bias, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    model.add(Conv2D(64, (3,3), activation='relu', kernel_initializer=initialize_weights,\n",
    "                     bias_initializer=initialize_bias, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(Conv2D(64, (3,3), activation='relu', kernel_initializer=initialize_weights,\n",
    "                     bias_initializer=initialize_bias, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='sigmoid',\n",
    "                   kernel_regularizer=l2(1e-3),\n",
    "                   kernel_initializer=initialize_weights,bias_initializer=initialize_bias))\n",
    "    \n",
    "    # Generate the encodings (feature vectors) for the two images\n",
    "    encoded_l = model(left_input)\n",
    "    encoded_r = model(right_input)\n",
    "    \n",
    "    # Add a customized layer to compute the absolute difference between the encodings\n",
    "    L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
    "    L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "    \n",
    "    # Add a dense layer with a sigmoid unit to generate the similarity score\n",
    "    prediction = Dense(1,activation='sigmoid',bias_initializer=initialize_bias)(L1_distance)\n",
    "    \n",
    "    # Connect the inputs with the outputs\n",
    "    siamese_net = Model(inputs=[left_input,right_input],outputs=prediction)\n",
    "    \n",
    "    # return the model\n",
    "    return siamese_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "704d5c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_11 (InputLayer)          [(None, 105, 105, 1  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " input_12 (InputLayer)          [(None, 105, 105, 1  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " sequential_5 (Sequential)      (None, 256)          1698880     ['input_11[0][0]',               \n",
      "                                                                  'input_12[0][0]']               \n",
      "                                                                                                  \n",
      " lambda_4 (Lambda)              (None, 256)          0           ['sequential_5[0][0]',           \n",
      "                                                                  'sequential_5[1][0]']           \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 1)            257         ['lambda_4[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,699,137\n",
      "Trainable params: 1,699,137\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_siamese_model((105, 105, 1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7b3866bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(learning_rate = 0.00006)\n",
    "model.compile(loss=\"binary_crossentropy\",optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c1370c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training alphabets: \n",
      "\n",
      "['Alphabet_of_the_Magi', 'Anglo-Saxon_Futhorc', 'Arcadian', 'Armenian', 'Asomtavruli_(Georgian)', 'Balinese', 'Bengali', 'Blackfoot_(Canadian_Aboriginal_Syllabics)', 'Braille', 'Burmese_(Myanmar)', 'Cyrillic', 'Early_Aramaic', 'Futurama', 'Grantha', 'Greek', 'Gujarati', 'Hebrew', 'Inuktitut_(Canadian_Aboriginal_Syllabics)', 'Japanese_(hiragana)', 'Japanese_(katakana)', 'Korean', 'Latin', 'Malay_(Jawi_-_Arabic)', 'Mkhedruli_(Georgian)', 'N_Ko', 'Ojibwe_(Canadian_Aboriginal_Syllabics)', 'Sanskrit', 'Syriac_(Estrangelo)', 'Tagalog', 'Tifinagh']\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(save_path, \"train.pickle\"), \"rb\") as f:\n",
    "    (Xtrain, train_classes) = pickle.load(f)\n",
    "    \n",
    "print(\"Training alphabets: \\n\")\n",
    "print(list(train_classes.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c6813607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation alphabets:\n",
      "\n",
      "['Angelic', 'Atemayar_Qelisayer', 'Atlantean', 'Aurek-Besh', 'Avesta', 'Ge_ez', 'Glagolitic', 'Gurmukhi', 'Kannada', 'Keble', 'Malayalam', 'Manipuri', 'Mongolian', 'Old_Church_Slavonic_(Cyrillic)', 'Oriya', 'Sylheti', 'Syriac_(Serto)', 'Tengwar', 'Tibetan', 'ULOG']\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(save_path, \"val.pickle\"), \"rb\") as f:\n",
    "    (Xval, val_classes) = pickle.load(f)\n",
    "\n",
    "print(\"Validation alphabets:\", end=\"\\n\\n\")\n",
    "print(list(val_classes.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "953bd5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch_size,s=\"train\"):\n",
    "    \"\"\"Create batch of n pairs, half same class, half different class\"\"\"\n",
    "    if s == 'train':\n",
    "        X = Xtrain\n",
    "        categories = train_classes\n",
    "    else:\n",
    "        X = Xval\n",
    "        categories = val_classes\n",
    "    n_classes, n_examples, w, h = X.shape\n",
    "\n",
    "    # randomly sample several classes to use in the batch\n",
    "    categories = rng.choice(n_classes,size=(batch_size,),replace=False)\n",
    "    \n",
    "    # initialize 2 empty arrays for the input image batch\n",
    "    pairs=[np.zeros((batch_size, h, w,1)) for i in range(2)]\n",
    "    \n",
    "    # initialize vector for the targets\n",
    "    targets=np.zeros((batch_size,))\n",
    "    \n",
    "    # make one half of it '1's, so 2nd half of batch has same class\n",
    "    targets[batch_size//2:] = 1\n",
    "    for i in range(batch_size):\n",
    "        category = categories[i]\n",
    "        idx_1 = rng.randint(0, n_examples)\n",
    "        pairs[0][i,:,:,:] = X[category, idx_1].reshape(w, h, 1)\n",
    "        idx_2 = rng.randint(0, n_examples)\n",
    "        \n",
    "        # pick images of same class for 1st half, different for 2nd\n",
    "        if i >= batch_size // 2:\n",
    "            category_2 = category  \n",
    "        else: \n",
    "            # add a random number to the category modulo n classes to ensure 2nd image has a different category\n",
    "            category_2 = (category + rng.randint(1,n_classes)) % n_classes\n",
    "        \n",
    "        pairs[1][i,:,:,:] = X[category_2,idx_2].reshape(w, h,1)\n",
    "    \n",
    "    return pairs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bc3e8aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(batch_size, s=\"train\"):\n",
    "    \"\"\"a generator for batches, so model.fit_generator can be used. \"\"\"\n",
    "    while True:\n",
    "        pairs, targets = get_batch(batch_size,s)\n",
    "        yield (pairs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "38aec9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_oneshot_task(N, s=\"val\", language=None):\n",
    "    \"\"\"Create pairs of test image, support set for testing N way one-shot learning. \"\"\"\n",
    "    if s == 'train':\n",
    "        X = Xtrain\n",
    "        categories = train_classes\n",
    "    else:\n",
    "        X = Xval\n",
    "        categories = val_classes\n",
    "    n_classes, n_examples, w, h = X.shape\n",
    "    \n",
    "    indices = rng.randint(0, n_examples,size=(N,))\n",
    "    if language is not None: # if language is specified, select characters for that language\n",
    "        low, high = categories[language]\n",
    "        if N > high - low:\n",
    "            raise ValueError(\"This language ({}) has less than {} letters\".format(language, N))\n",
    "        categories = rng.choice(range(low,high),size=(N,),replace=False)\n",
    "\n",
    "    else: # if no language specified just pick a bunch of random letters\n",
    "        categories = rng.choice(range(n_classes),size=(N,),replace=False)            \n",
    "    true_category = categories[0]\n",
    "    ex1, ex2 = rng.choice(n_examples,replace=False,size=(2,))\n",
    "    test_image = np.asarray([X[true_category,ex1,:,:]]*N).reshape(N, w, h,1)\n",
    "    support_set = X[categories,indices,:,:]\n",
    "    support_set[0,:,:] = X[true_category,ex2]\n",
    "    support_set = support_set.reshape(N, w, h,1)\n",
    "    targets = np.zeros((N,))\n",
    "    targets[0] = 1\n",
    "    targets, test_image, support_set = shuffle(targets, test_image, support_set)\n",
    "    pairs = [test_image,support_set]\n",
    "\n",
    "    return pairs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e20e5940",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_oneshot(model, N, k, s = \"val\", verbose = 0):\n",
    "    \"\"\"Test average N way oneshot learning accuracy of a siamese neural net over k one-shot tasks\"\"\"\n",
    "    n_correct = 0\n",
    "    if verbose:\n",
    "        print(\"Evaluating model on {} random {} way one-shot learning tasks ... \\n\".format(k,N))\n",
    "    for i in range(k):\n",
    "        inputs, targets = make_oneshot_task(N,s)\n",
    "        probs = model.predict(inputs)\n",
    "        if np.argmax(probs) == np.argmax(targets):\n",
    "            n_correct+=1\n",
    "    percent_correct = (100.0 * n_correct / k)\n",
    "    if verbose:\n",
    "        print(\"Got an average of {}% {} way one-shot learning accuracy \\n\".format(percent_correct,N))\n",
    "    return percent_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e29331",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f50aa172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "evaluate_every = 200 # interval for evaluating on one-shot tasks\n",
    "batch_size = 32\n",
    "n_iter = 20000 # No. of training iterations\n",
    "N_way = 20 # how many classes for testing one-shot tasks\n",
    "n_val = 250 # how many one-shot tasks to validate on\n",
    "best = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8e881eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './weights/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d9f32c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 200 iterations: 1.3722941398620605 mins\n",
      "Train Loss: 0.5912116169929504\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 16.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 16.0, previous best: -1\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 400 iterations: 2.9054863810539246 mins\n",
      "Train Loss: 0.5640565752983093\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 25.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 25.2, previous best: 16.0\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 600 iterations: 4.5163977225621545 mins\n",
      "Train Loss: 0.6277018189430237\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 27.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 27.2, previous best: 25.2\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 800 iterations: 6.175065799554189 mins\n",
      "Train Loss: 0.5398242473602295\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 24.8% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 1000 iterations: 7.825952780246735 mins\n",
      "Train Loss: 0.6057655215263367\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 28.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 28.8, previous best: 27.2\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 1200 iterations: 9.383279156684875 mins\n",
      "Train Loss: 0.45940086245536804\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 24.8% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 1400 iterations: 10.943830156326294 mins\n",
      "Train Loss: 0.5458990335464478\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 29.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 29.2, previous best: 28.8\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 1600 iterations: 12.603369510173797 mins\n",
      "Train Loss: 0.5188636779785156\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 29.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 29.2, previous best: 29.2\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 1800 iterations: 14.138609460989635 mins\n",
      "Train Loss: 0.4083757996559143\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 30.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 30.0, previous best: 29.2\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 2000 iterations: 15.670386473337809 mins\n",
      "Train Loss: 0.604826807975769\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 31.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 31.2, previous best: 30.0\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 2200 iterations: 17.20639352798462 mins\n",
      "Train Loss: 0.5375843048095703\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 30.8% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 2400 iterations: 18.713201769193013 mins\n",
      "Train Loss: 0.4883808493614197\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 33.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 33.6, previous best: 31.2\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 2600 iterations: 20.235477030277252 mins\n",
      "Train Loss: 0.5070600509643555\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 34.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 34.0, previous best: 33.6\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 2800 iterations: 21.750973359743753 mins\n",
      "Train Loss: 0.34420838952064514\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 32.0% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 3000 iterations: 23.25395212173462 mins\n",
      "Train Loss: 0.42957061529159546\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 30.0% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 3200 iterations: 24.90792869726817 mins\n",
      "Train Loss: 0.45226773619651794\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 32.0% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 3400 iterations: 26.631106658776602 mins\n",
      "Train Loss: 0.42285117506980896\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 32.8% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 3600 iterations: 28.29463461637497 mins\n",
      "Train Loss: 0.3341699242591858\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 30.4% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 3800 iterations: 29.93797338803609 mins\n",
      "Train Loss: 0.5151692032814026\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 32.4% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 4000 iterations: 31.599941917260487 mins\n",
      "Train Loss: 0.38394874334335327\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 36.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 36.8, previous best: 34.0\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 4200 iterations: 33.22904232343038 mins\n",
      "Train Loss: 0.3696177005767822\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 34.4% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 4400 iterations: 34.87832133372625 mins\n",
      "Train Loss: 0.38828200101852417\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 34.8% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 4600 iterations: 36.51600547234217 mins\n",
      "Train Loss: 0.5111697316169739\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 39.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 39.2, previous best: 36.8\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 4800 iterations: 38.14785513480504 mins\n",
      "Train Loss: 0.43301132321357727\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 31.6% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 5000 iterations: 39.809104863802595 mins\n",
      "Train Loss: 0.6075847148895264\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 36.0% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 5200 iterations: 41.451001183191934 mins\n",
      "Train Loss: 0.30366912484169006\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 40.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 40.4, previous best: 39.2\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 5400 iterations: 43.06625378926595 mins\n",
      "Train Loss: 0.29448238015174866\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 38.0% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 5600 iterations: 44.69937003850937 mins\n",
      "Train Loss: 0.30848464369773865\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 38.4% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 5800 iterations: 46.35326062043508 mins\n",
      "Train Loss: 0.3987266719341278\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 45.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 45.6, previous best: 40.4\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 6000 iterations: 48.04330048163732 mins\n",
      "Train Loss: 0.362282395362854\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 41.2% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 6200 iterations: 49.69952950874964 mins\n",
      "Train Loss: 0.3044966459274292\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 44.0% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 6400 iterations: 51.335411643981935 mins\n",
      "Train Loss: 0.4785682260990143\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 43.6% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 6600 iterations: 52.99312860965729 mins\n",
      "Train Loss: 0.3845052421092987\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 48.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 48.4, previous best: 45.6\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 6800 iterations: 54.627735455830894 mins\n",
      "Train Loss: 0.33638548851013184\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got an average of 48.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 48.4, previous best: 48.4\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 7000 iterations: 56.2604888399442 mins\n",
      "Train Loss: 0.32548987865448\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 48.0% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 7200 iterations: 57.78954239288966 mins\n",
      "Train Loss: 0.28444167971611023\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 54.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 54.0, previous best: 48.4\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 7400 iterations: 59.41097561120987 mins\n",
      "Train Loss: 0.3224620521068573\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 48.0% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 7600 iterations: 61.00941954453786 mins\n",
      "Train Loss: 0.29720139503479004\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 49.6% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 7800 iterations: 62.640553108851115 mins\n",
      "Train Loss: 0.3338082432746887\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 51.6% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 8000 iterations: 64.26692151228586 mins\n",
      "Train Loss: 0.34115612506866455\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 56.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 56.0, previous best: 54.0\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 8200 iterations: 65.88111661275228 mins\n",
      "Train Loss: 0.35836777091026306\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 49.6% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 8400 iterations: 67.51220316092173 mins\n",
      "Train Loss: 0.22756989300251007\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 55.6% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 8600 iterations: 69.1308241168658 mins\n",
      "Train Loss: 0.2554268538951874\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 54.0% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 8800 iterations: 70.7778245886167 mins\n",
      "Train Loss: 0.375516414642334\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 54.8% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 9000 iterations: 72.46701338291169 mins\n",
      "Train Loss: 0.29032251238822937\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 54.0% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 9200 iterations: 74.10389787753424 mins\n",
      "Train Loss: 0.3250221014022827\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 54.0% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 9400 iterations: 75.75065145889918 mins\n",
      "Train Loss: 0.4112683832645416\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 51.6% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 9600 iterations: 77.40932349761327 mins\n",
      "Train Loss: 0.3266358971595764\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 53.6% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 9800 iterations: 79.01668286720911 mins\n",
      "Train Loss: 0.35606029629707336\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 54.4% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 10000 iterations: 80.70634060700735 mins\n",
      "Train Loss: 0.25069358944892883\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 59.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 59.2, previous best: 56.0\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 10200 iterations: 82.33262209097545 mins\n",
      "Train Loss: 0.20138542354106903\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 62.0, previous best: 59.2\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 10400 iterations: 83.99358790715536 mins\n",
      "Train Loss: 0.2647855579853058\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 59.6% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 10600 iterations: 85.64959956804911 mins\n",
      "Train Loss: 0.18602484464645386\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 60.8% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 10800 iterations: 87.33656823237737 mins\n",
      "Train Loss: 0.3135799467563629\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 58.4% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 11000 iterations: 88.95791521469752 mins\n",
      "Train Loss: 0.20280054211616516\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 57.2% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 11200 iterations: 90.63717190821966 mins\n",
      "Train Loss: 0.24716146290302277\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 61.6% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 11400 iterations: 92.28673942486445 mins\n",
      "Train Loss: 0.19833356142044067\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 64.4, previous best: 62.0\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 11600 iterations: 93.93513535658518 mins\n",
      "Train Loss: 0.2146412581205368\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 63.2% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 11800 iterations: 95.61596628427506 mins\n",
      "Train Loss: 0.28042590618133545\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 60.4% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 12000 iterations: 97.40784078836441 mins\n",
      "Train Loss: 0.19452638924121857\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 60.8% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 12200 iterations: 99.22140598694483 mins\n",
      "Train Loss: 0.1658370941877365\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 58.8% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 12400 iterations: 100.95462733109792 mins\n",
      "Train Loss: 0.22335393726825714\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 60.0% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 12600 iterations: 102.60468014876048 mins\n",
      "Train Loss: 0.3291541039943695\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 55.6% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 12800 iterations: 104.40220034122467 mins\n",
      "Train Loss: 0.2712065577507019\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 60.0% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 13000 iterations: 106.0393892288208 mins\n",
      "Train Loss: 0.19159209728240967\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 54.8% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 13200 iterations: 107.72771093050639 mins\n",
      "Train Loss: 0.17709773778915405\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 60.4% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 13400 iterations: 109.37531232436498 mins\n",
      "Train Loss: 0.20809096097946167\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 67.2, previous best: 64.4\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 13600 iterations: 111.07505839665731 mins\n",
      "Train Loss: 0.48267561197280884\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 63.6% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 13800 iterations: 112.75881474018097 mins\n",
      "Train Loss: 0.28827938437461853\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got an average of 65.2% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 14000 iterations: 114.47629321813584 mins\n",
      "Train Loss: 0.21457603573799133\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 56.8% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 14200 iterations: 116.11717731157938 mins\n",
      "Train Loss: 0.12269774079322815\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.4% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 14400 iterations: 117.81877036094666 mins\n",
      "Train Loss: 0.22173547744750977\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.4% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 14600 iterations: 119.63344022432963 mins\n",
      "Train Loss: 0.22026373445987701\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 63.2% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 14800 iterations: 121.3086671590805 mins\n",
      "Train Loss: 0.15331073105335236\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.0% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 15000 iterations: 123.02990370988846 mins\n",
      "Train Loss: 0.16081362962722778\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 58.0% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 15200 iterations: 124.75391675631205 mins\n",
      "Train Loss: 0.257952481508255\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.0% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 15400 iterations: 126.44035175244014 mins\n",
      "Train Loss: 0.2893684208393097\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 61.6% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 15600 iterations: 128.11205919981003 mins\n",
      "Train Loss: 0.1845056414604187\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.0% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 15800 iterations: 129.7512294014295 mins\n",
      "Train Loss: 0.17043839395046234\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.0% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 16000 iterations: 131.42289584875107 mins\n",
      "Train Loss: 0.20170067250728607\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 63.6% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 16200 iterations: 133.07641607522964 mins\n",
      "Train Loss: 0.3830571174621582\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 67.6, previous best: 67.2\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 16400 iterations: 134.7437797307968 mins\n",
      "Train Loss: 0.23922458291053772\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.4% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 16600 iterations: 136.42791470686595 mins\n",
      "Train Loss: 0.1642235815525055\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 58.4% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 16800 iterations: 138.07207280397415 mins\n",
      "Train Loss: 0.16397437453269958\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.2% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 17000 iterations: 139.75540337959924 mins\n",
      "Train Loss: 0.20916879177093506\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 67.6, previous best: 67.6\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 17200 iterations: 141.4261383374532 mins\n",
      "Train Loss: 0.15050345659255981\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.2% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 17400 iterations: 143.14603191216787 mins\n",
      "Train Loss: 0.22601597011089325\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.8% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 17600 iterations: 144.88604472080866 mins\n",
      "Train Loss: 0.15941017866134644\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 68.8, previous best: 67.6\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 17800 iterations: 146.44996690750122 mins\n",
      "Train Loss: 0.328325092792511\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 69.6, previous best: 68.8\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 18000 iterations: 148.06658970514934 mins\n",
      "Train Loss: 0.13699392974376678\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.6% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 18200 iterations: 149.80513563553492 mins\n",
      "Train Loss: 0.2588977813720703\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 63.2% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 18400 iterations: 151.47671658198038 mins\n",
      "Train Loss: 0.15835240483283997\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.2% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 18600 iterations: 153.281360856692 mins\n",
      "Train Loss: 0.1924024373292923\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 73.2, previous best: 69.6\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 18800 iterations: 154.9178837776184 mins\n",
      "Train Loss: 0.2064036726951599\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.8% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 19000 iterations: 156.5167325894038 mins\n",
      "Train Loss: 0.1441904902458191\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.6% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 19200 iterations: 158.14962058464687 mins\n",
      "Train Loss: 0.15263839066028595\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 63.2% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 19400 iterations: 159.71883008480071 mins\n",
      "Train Loss: 0.17568059265613556\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.4% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 19600 iterations: 161.29792534112931 mins\n",
      "Train Loss: 0.21046072244644165\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 63.2% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 19800 iterations: 162.87346506913502 mins\n",
      "Train Loss: 0.17943459749221802\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.4% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 20000 iterations: 164.44526861111322 mins\n",
      "Train Loss: 0.10513824224472046\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.8% 20 way one-shot learning accuracy \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"-------------------------------------\")\n",
    "t_start = time.time()\n",
    "for i in range(1, n_iter+1):\n",
    "    (inputs,targets) = get_batch(batch_size)\n",
    "    loss = model.train_on_batch(inputs, targets)\n",
    "    if i % evaluate_every == 0:\n",
    "        print(\"\\n ------------- \\n\")\n",
    "        print(\"Time for {0} iterations: {1} mins\".format(i, (time.time()-t_start)/60.0))\n",
    "        print(\"Train Loss: {0}\".format(loss)) \n",
    "        val_acc = test_oneshot(model, N_way, n_val, verbose=True)\n",
    "        model.save_weights(os.path.join(model_path, 'weights.{}.h5'.format(i)))\n",
    "        if val_acc >= best:\n",
    "            print(\"Current best: {0}, previous best: {1}\".format(val_acc, best))\n",
    "            best = val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1985c578",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
